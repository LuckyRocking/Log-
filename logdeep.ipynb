{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "from torch.nn import LSTM,RNN,GRU,BatchNorm1d,Linear,Flatten,Dropout,Conv1d\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "#定义模型\n",
    "class logdifmodel_basebert(nn.Module):\n",
    "    def __init__(self,input_shape:int) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm1=LSTM(input_size=input_shape,hidden_size=64)\n",
    "        self.lstm2=LSTM(input_size=64,hidden_size=32)\n",
    "        self.flatten=Flatten()\n",
    "        self.dropout=Dropout(0.1)\n",
    "        self.dense1=Linear(3200,15936,bias=True)\n",
    "    \n",
    "    #前向传播\n",
    "    def forward(self,x):\n",
    "        out,(hidden,c)=self.lstm1(x)\n",
    "        out,(hidden,c)=self.lstm2(out)\n",
    "        flatten_out=self.flatten(out)\n",
    "        drop_out=self.dropout(flatten_out)\n",
    "        output=F.softmax(self.dense1(drop_out))\n",
    "        return output\n",
    " \n",
    "def get_bestmodel(input_size:int,train_x:torch.tensor,train_y:torch.tensor,learning_rate=0.1,beta=(0.9,0.98),epochs=100,batch_size=128):\n",
    "\n",
    "    #train_x,train_y为torch.tensor类型，\n",
    "    logdif_model=logdifmodel_basebert(input_size)\n",
    "    #定义损失函数\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    #定义优化器\n",
    "    optimizer=optim.Adam(logdif_model.parameters(),lr=learning_rate,betas=beta)\n",
    "    #确定多少个batch\n",
    "    n_batchs=len(train_x)//batch_size\n",
    "    #开始进行训练\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for n_batch in range(n_batchs):\n",
    "            batch_input=train_x[n_batchs*batch_size:n_batchs*batch_size+batch_size]\n",
    "            batch_labels=train_y[n_batchs*batch_size:n_batchs*batch_size+batch_size]\n",
    "            batch_output=logdif_model(batch_input)\n",
    "            #计算损失\n",
    "            loss=criterion(batch_output,batch_labels)\n",
    "            #将梯度重置为0\n",
    "            optimizer.zero_grad()\n",
    "            #反向梯度传播\n",
    "            loss.backward()\n",
    "            #优化参数\n",
    "            optimizer.step()\n",
    "            #计算正确率\n",
    "            y_pre=torch.max(logdif_model(train_features),1)[1]\n",
    "            acc=float((y_pre==labels).sum())/float(labels.shape[0])\n",
    "            print(str(loss),'acc of this model is :%.4f'%acc)\n",
    "    return logdif_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import Sequential,layers\n",
    "from keras.layers import Dense,LSTM,SimpleRNN,Flatten\n",
    "\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(LSTM(64,return_sequences=True,input_shape=(100,768)))\n",
    "#model.add(LSTM(32,activation='tanh'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15936,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics='acc')\n",
    "model.fit(keras_data,keras_labels,epochs=2,batch_size=20,validation_data=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e450050b432e843bda3c41bf3272c133bfc370a7003f3e377e27f87a49ce1127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
